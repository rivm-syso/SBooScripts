---
title: "Solver script"
author: "Valerie de Rijk, Joris Quik, Jaap Slootweg"
date: "`r Sys.Date()`"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = '/home/rijkdv/git_folder/SBooScripts')
```

## Initiation

We assume you have the input data for a substance or material of interest and all the data describing the SimpleBox world to be created ready and thus can run the initWorld script.

```{r}
library(dplyr)
substance <-  "GO-Chitosan"
source("baseScripts/initWorld_onlyParticulate.R")
World$fetchData("RhoS")
World$fetchData("RadS")


```

## Computing Spherical equivalent diameter

We calculate the spherical equivalent diameter (deq) and subsequently use it to overwrite radS. In this manner we include the shape of the considered particles. We update the matrix in the chunk after. [TODO: In future this could be included in the initialization for relevant particles that consist of multiple components]

We need the following properties for the GO-Chitosan related particles:

-   Shape

-   Size

-   Density

-   Other 'unknown' variables, such as attachment efficiency, etc.

| Property             | GO-Chitosan     | GO              | Chitoson      |
|----------------------|-----------------|-----------------|---------------|
| Shape                | Sheet-like      | Flake           | Fragment      |
| Size - square (LxB)  | 70 - 90 (80) um | 60 - 70 (65) um | 10-20 (10) um |
| Size - thickness (H) | 10-20 (15) um   | 1-10 (5) um     | 10-20 (10) um |
| Density              | Calculated      | 0.35 g/ml       | 0.874 g/ml    |

: The density of GO-chitosan is approximated by 7/8 \* dens_Graphene + 1/8 \* dens_Chitosan

```{r}

Longest <- World$fetchData("Longest_side")
Intermediate <- World$fetchData("Intermediate_side")
Shortest <- World$fetchData("Shortest_side")
Volume <- Longest*Intermediate*Shortest
d_eq <- ( 6/ pi * Volume)^(1/3)
rad_eq <- d_eq/2
print(rad_eq)

World$SetConst(RadS = rad_eq)

World$fetchData("RhoS")
```

## Adjusting Parameters with Uncertainty

Since attachment efficiencies (alpha) are very uncertain, below is a chunk where we can create distributions for these parameters. We start however with a deterministic calculation using averages.

```{r Creating adjustments for unknown alpha parameters}
#non-marine
log_min <- -3
log_max<- -1
n <- 100000
log_uniform_samples <- 10^runif(n, min = log10(10^log_min), max = log10(10^log_max))
fw_alpha_mean_log_samples <- mean(log_uniform_samples)
print(fw_alpha_mean_log_samples)

#Check with histogram 
hist(log_uniform_samples, breaks = 30, freq = FALSE,
     main = "Histogram of Log Uniform Distribution [10^-3, 10^-1]",
     xlab = "Value", ylab = "Density")
# Plot the probability density function (pdf) curve
curve(dunif(log10(x), min = log10(10^log_min), max = log10(10^log_max)) / x,
      from = 10^log_min, to = 10^log_max, add = TRUE, col = "blue", lwd = 2 )
#marine 
log_min <- -0.1 
log_max <- -0.0001
log_uniform_samples <- 10^runif(n, min = log10(10^log_min), max = log10(10^log_max))
marine_alpha_mean_log_samples <- mean(log_uniform_samples)
print(marine_alpha_mean_log_samples)

#Assign data
World$fetchData("alpha")
subcomparts <- c("river", "lake", "water", "agriculturalsoil", "naturalsoil", "othersoil", "freshwatersediment", "lakesediment")
alpha_adjust_fw <- data.frame(
  SubCompart = subcomparts, 
  alpha = fw_alpha_mean_log_samples
  
)

World$mutateVar(alpha_adjust_fw)

subcompartsmarine <- c("sea", "marinesediment", "freshwatersediment", "deepocean")
alpha_adjust_marine <- data.frame(
  SubCompart = subcompartsmarine, 
  alpha = marine_alpha_mean_log_samples
  
)
World$mutateVar(alpha_adjust_marine)
World$UpdateKaas(mergeExisting = F)

```

## Adjusting Parameters to Match considered Scale

In this case study we are considering emission data only for Europe. By default the 'World' is represented by a nested regional scale, which is not relevant for the current assessment using emissions data only for Europe. Here we use the option to allocate part of the emissions to the regional scale based on the fraction of surface area in order to mimic not having a nested scale. In future one would be interested for instance in including a local or national scale as well. One could adjust the regional scale for this purpose.

The code is commented out, but this is an example of adjusting the regional scale to represent Switzerland. You can only adjust parameters that are initial input data, not variables that are calculated later in SBOO. The adjusted dataframes are printed below. Note, at this point the input is already converted to SI units, so new data also needs to be put in this format.

```{r}
# 
# LandFRAC <- data.frame(
#   Scale = "Regional",
#   SubCompart = c("agriculturalsoil", "lake", "naturalsoil", "othersoil" , "river"), 
#   landFRAC = c(0.37, 0.02, 0.51, 0.08, 0.02)
# )
# # TotalArea <- data.frame(
# #   Scale = c("Arctic", "Continental", "Moderate", "Regional"),
# #   TotalArea = c(4.25E+13, 7.43E+12, 8.50E+13, 4.13e+11)
# # )
# 
# Temperature <- data.frame(
#   Scale = "Regional", 
#   Temp = 279
# )
# 
# RAINrate <- data.frame(
#   Scale = "Arctic",
#   RAINrate = 4.37e-5
# )
# 
# ParamToAdjust <- list(LandFRAC, Temperature, RAINrate)
# for (i in seq_along(ParamToAdjust)) {
#   ParamToAdjust[[i]] <- World$mutateVar(ParamToAdjust[[i]])
# }
# print(ParamToAdjust)

## scaling of world 

Area <- World$fetchData("TotalArea")
AreaRegional <- Area$TotalArea[Area$Scale =="Regional"]
AreaContinental <- Area$TotalArea[Area$Scale =="Continental"]
fracReg <- AreaRegional/AreaContinental
fracCont <- 1-fracReg

FracRC <- tibble(
  Scale_SBname = c("Regional","Continental"),
  Abr_scale = c("R","C"),
  AreaFraction = c(fracReg,fracCont)
)


print(FracRC)

```

## NewSolver

Different solvers are available, basically:

1.  Solving the steadystate of the SimpleBox world

2.  Solving in time the states of SimpleBox world

Both will be illustrated bellow, but it starts with defining the solver you want to use by `world$NewSolver("[name of s_function]")`

### SBsteady

```{r SBsteady}
World$NewSolver("SBsteady")
```

What solving means is that using matrix algebra a set of differential equations is solved:

`K %*% m + e`

Where:

K is the matrix of rate constants for each process describing the mass transfers to and from and out of a state (e.g. substance in freshwater (w1U) or small heteroagglomerate in natural soil (s1A)).

m is the mass in each compartment, e.g. 0 at t=0.

e is the emission to each compartment per unit of time, e.g. 1 t/y.

To solve this set of differential equations we thus need an emission, e.g. 1 ton/year to air. The height of this emission is not

```{r constant emission}
emissions <- data.frame(Abbr = "aCS", Emis = 1000/(365.25*24*60*60)) # convert 1 t/y to si units: kg/s

# TODO: explain what is the reason for this Abbr? Why is it not a relational table defining scale, compartment and species as for all other data?
```

Now we are ready to run the solver, which results in the mass in each compartment.

```{r}
World$Solve(emissions)
```

## SBdynamic

We can also solve the differential equations dynamically in time, but the optimal implementation is still work in progress, see [issue](https://github.com/rivm-syso/SBoo/issues/111).

```{r emission data}
file_paths <- 
  list.files("data/emissions",recursive = TRUE)
Emissions <-
  read_csv(paste0("data/emissions/",file_paths), id="file_name", col_names = c("RUN",0:24),skip = 1) # unit: Metric tonnes


```

### Prepare DPMFA data

Data from an DPMFA model should be prepared to fit the SBoo world. For instance the time unit should be correct, the mass unit is not as important as this will be the same in the output then, but for good measure we use kg. This is the quick and dirty way, a more elegant way is till in progress as mentioned above.

We define the compartments of the emission based on the DMPFA model.

```{r Emissions}
# TODO: solve for every RUN
Emissions <- 
  Emissions |>
  pivot_longer(
    cols = !c(file_name,RUN),
    names_to = "year",
    values_to = "emission_t" ) |> mutate_at('year',as.numeric) |> 
  ungroup() |> 
  group_by(file_name,year) |> 
  summarise(Emission_p50_kg = quantile(emission_t,probs = 0.5)*1000,
            Emission_mean_kg = mean(emission_t)*1000) |> ungroup()

Emissions <- 
  Emissions |> 
  mutate(compartment = 
           case_when(str_detect(file_name, "(?i)Air") ~ "Air",
                     str_detect(file_name, "(?i)Soil") ~ "SludgeTreatedSoil",
                     str_detect(file_name, "(?i)Water") ~ "SurfaceWater",
                     str_detect(file_name, "(?i)Subsurface") ~ "Subsurface",
                     TRUE ~ "Other"),
         scale = 
           case_when(str_detect(file_name, "(?i)EU") ~ "EU_average",
                     str_detect(file_name, "(?i)Ireland") ~ "EU_STsoil",
                     str_detect(file_name, "(?i)Switzerland") ~ "EU_noSTsoil",
                     TRUE ~ "Other"),
         Substance = "GO-Chitosan",
  )

```

### Scaling Emission data based on material density

When running for only GO or only Chitosan, you want to correct for the fact that it's partly Chitosan and partly GO through using the densities.

```{r DensityScaling}

Weightfactor <- switch(substance,
                       "GO-Chitosan" = 1,
                       "Chitosan" = 7/8,
                       "GO" = 1/8)

Emissions <-
  Emissions |> mutate(Emission_mean_kg = Emission_mean_kg* Weightfactor,
                      Emission_p50_kg = Emission_p50_kg*Weightfactor)

head(Emissions)

```

### Scaling Input Data based on World (Regional nested in Continental)

In this chunk, we adjust for the fact that the input data for the DMPFA model is all Europe based. Hence we scale by the factor fracReg and fracCont to still include the current regional scale (could alse be done differently). The regional data is thus a portion of the EU emission, scaled based on the land surface area.

```{r ScaleScaling}

SBEmissions2 <- 
  Emissions |> mutate(
    Abr_comp =  case_match(compartment,
                           "Air" ~ "a",
                           "SludgeTreatedSoil" ~ "s2",
                           "SurfaceWater" ~ "w1",
                           .default = NA
    ),
    Abr_scale =  case_match(scale,
                            "EU_average" ~ "C",
                            .default = NA
    ),
    Abr_species = "S"
    
  ) |> drop_na() 

SBEmissions2 <-  
  SBEmissions2 |> rbind(
    SBEmissions2 |> 
      mutate(
        Abr_scale =  case_match(Abr_scale,
                                "C" ~ "R",
                                .default = NA
        ),
        scale = "EU_average"
      )
  ) |> full_join(FracRC) |> 
  
  mutate(Abr = paste0(Abr_comp,Abr_scale,Abr_species)) |> 
  mutate(Emission_mean_kg = Emission_mean_kg*AreaFraction,
         Emission_p50_kg = Emission_p50_kg*AreaFraction)

head(SBEmissions2)

```

### make time dependant emission functions

```{r approxfuns}
SBEmissions3 <- 
  SBEmissions2 |> 
  mutate(time_s = year*(365.25*24*60*60)+(365.25*24*60*60)) |> ungroup() |> 
  group_by(compartment,Abr_scale,Abr,Substance) |> 
  summarise(n=n(),
            EmisFun = list(
              approxfun(
                data.frame(time_s = c(0,time_s), 
                           emis_kg=c(0,Emission_mean_kg)),
                rule = 1:1)
            )
  )

funlist <- SBEmissions3$EmisFun
names(funlist) <- SBEmissions3$Abr

times <- seq(0, 25*365.25*24*3600, by = 10000)

CompartInterest <- "w1CS"

time_s = c(0,(SBEmissions2 |> filter(Abr == CompartInterest) |> pull(year))*(365.25*24*60*60)+(365.25*24*60*60))
emis_kg = c(0,(SBEmissions2 |> filter(Abr == CompartInterest) |> pull(Emission_mean_kg)))

PlotEmisFun = funlist[[CompartInterest]]

plot(time_s,
     emis_kg)
curve(PlotEmisFun,
      add = TRUE)

```


## Dynamic solving for deterministic input data

The chunk below gives the opportunity to Solve for constant input data. The chunk after that gives the opportunity to also vary SB Input data

```{r DynamicSolve}

##ODE

SimpleBoxODE = function(t, m, parms) {
  
  with(as.list(c(parms, m)), {
    e <- c(rep(0, length(SBNames)))
    for (name in names(funlist)) {
      e[grep(name, SBNames)] <- funlist[[name]](t)
    }
    dm <- K%*% m + e
    res <- c(dm)
    list(res, signal = e)
  })
}

# print(SBNames)

#Function to Solve
SBsolve4 <- function( tmax = 1e10, nTIMES = 100, Engine, funlist) {
  
  SB.K = Engine
  SBNames = colnames(Engine)
  SB.m0 <- rep(0, length(SBNames))
  SBtime <- seq(0,tmax,length.out = nTIMES)
  
  
  out <- deSolve::ode(
    y = as.numeric(SB.m0),
    times = SBtime ,
    func = SimpleBoxODE,
    parms = list(K = SB.K, SBNames=SBNames, funlist=funlist),
    rtol = 1e-10, atol = 1e-2)
  #if(as.character(class(deS)[1])!="data.frame") return (list(errorstate="error", deS))
  colnames(out)[1:length(SBNames)+1] <- SBNames
  colnames(out)[grep("signal",colnames(out))] <- paste("emis",SBNames,sep = "2")
  as.data.frame(out)
  
}
#Solving
Solution <- SBsolve4(tmax = 25*(365.25*24*3600), 
                     nTIMES = 130,
                     Engine = World$exportEngineR(),
                     funlist = funlist)
Solution <- as.data.frame(Solution)

# Same plot of emission as above
plot(Solution$time,Solution$emis2w1CS)

plot(Solution$time,Solution$w1CS)

plot(Solution$time,Solution$s1CS)

plot(Solution$time,Solution$aCS)

```

## Varying Input Data

This is still work in progress. To try multiple definitions of input parameters, you currently have to extract the calculation engine from the R6 core with different input.This chunk gives the possibility to solve engines for (combinations of) varing particles and solve for the varying input data with the code chunck below that. In this chunk, a check for the module k_Sedimentation is implemented as a check for dependency on particle size.

```{r Input data, eval = FALSE}
particle_sizes <- seq(from = 50e-6, to = 90e-6, length.out = 10)
#Generation of matrix 
result_engine <- list()
result_sedimentation <- list()


#function for getting engines
run_particle_simulation <- function(particle_sizes, emissions) {
  # Function to run simulation for a single particle size
  run_simulation <- function(size) {
    print(paste("Running simulation for particle size:", size))
    World$SetConst(RadS = size)
    World$UpdateKaas(mergeExisting = FALSE)
    
    sedimentation <- World$moduleList[["k_Sedimentation"]]$execute()
    sedimentation$particle_size <- size
    
    World$NewSolver("SBsteady")
    World$Solve(emissions)
    Engine <- World$exportEngineR()
    
    result <- list(
      engine_result = Engine,
      sedimentation_result = sedimentation
    )
    
    return(result)
  }
  
  # Use lapply to iterate over particle sizes
  results <- suppressWarnings(lapply(particle_sizes, run_simulation))
  
  # Split results into separate lists for engine and sedimentation
  result_engine <- lapply(results, function(res) res$engine_result)
  result_sedimentation <- lapply(results, function(res) res$sedimentation_result)
  
  # Return results as a list
  return(list(
    engine_results = result_engine,
    sedimentation_results = result_sedimentation
  ))
}

# Call the function
simulation_results <- run_particle_simulation(particle_sizes, emissions)
sedimentation_results <- simulation_results$sedimentation_results
engine_results <- simulation_results$engine_results

```

## Solver Module

```{r Solver, eval = FALSE}
SimpleBoxODE = function(t, m, parms) {
  
  with(as.list(c(parms, m)), {
    e <- c(rep(0, length(SBNames)))
    for (name in names(interpolations)) {
      e[grep(name, SBNames)] <- interpolations[[name]](t)
    }
    dm <- K%*% m + e
    res <- c(dm)
    list(res, signal = e)
  })
}

# exporting Engine
Engine_1 <- engine_results[[1]]
SBNames <-colnames(Engine_1)
print(SBNames)
print(SBNames)
SB.m0 <- rep(0, length(SBNames))
print(length(SB.m0))
interpolations <-funlist
SBsolve4 <- function(tmax = 1e10, nTIMES = 100, Engine) {
  
  SB.K <- Engine
  
  SBtime <- seq(0, tmax, length.out = nTIMES)
  
  out <- deSolve::ode(
    y = as.numeric(SB.m0),
    times = SBtime,
    func = SimpleBoxODE,
    parms = list(K = SB.K, SBNames, interpolations),
    rtol = 1e-10, atol = 1e-2
  )
  
  return(out)
}

# Initialize a list to store the solutions
Solutions <- list()

# Loop through each Engine and solve
for (i in seq_along(engine_results)) {
  print(i)
  Solution <- SBsolve4(tmax = 25 * (365.25 * 24 * 3600), nTIMES = 10, Engine = engine_results[[i]])
  Solutions[[i]] <- Solution
  rm(Solution)
}

```

## Output Processing

To move from the solution of the ODE solver to usable output data we need to split the output into corresponding mass data per compartment over time and emission signals over time. The first chunk is for one output, the latter chunk for varying input or emission data.

```{r Output Processing one solution}
library(dplyr)
#for one Solution
Solution <- as.data.frame(Solution)
Engine <- World$exportEngineR()
NamesK <- colnames(Engine)
print(NamesK)
print(length(NamesK))
colnames(Solution)[2:156] <- NamesK

#seperate signals and matrix
#compartments
compartments <- Solution[, 1:156]
compartments <- compartments |> select(-matches("U$"))
signals <- Solution[, 157:311]
colnames(signals) <- NamesK
signals_total <- data.frame(rowSums(signals))
signals_total$time <- Solution[, 1]

#Checks
#Emission over time
plot1 <-ggplot(signals_total, aes(x = time, y =rowSums.signals.)) +
  geom_line() +
  labs(title = "Emissions Over Time", x = "Time", y = "Emissions")

show(plot1)

#further data manipulation for the compartments
split_df <- function(df, solution_matrix) {
  # Extract the capital letters (A, R, C, T, M) in column names
  patterns <- unique(gsub("[^ARCTM]", "", colnames(df)))
  
  # Function to adjust the dataframe
  adjust_df <- function(df) {
    df$time <- compartments$time
    return(df)
  }
  
  # Split the dataframe based on the presence of specified capital letters using lapply
  split_dfs <- lapply(patterns, function(pattern) {
    cols <- grepl(pattern, colnames(df))
    adjusted_df <- df[, cols, drop = FALSE]
    adjust_df(adjusted_df)
  })
  
  # Assign names to the split dataframes
  names(split_dfs) <- patterns
  
  # Output the split dataframes
  invisible(lapply(names(split_dfs), function(pattern) {
    cat("Pattern:", pattern, "\n")
    print(split_dfs[[pattern]])
    cat("\n")
  }))
  
  return(split_dfs)
}

# Apply the function to split the dataframe
split_dfs <- split_df(compartments, solution_matrix)

# Assuming A, C, T, R, and M are the split dataframes
A <- split_dfs[["A"]]
C <- split_dfs[["C"]]
T <- split_dfs[["T"]]
R <- split_dfs[["R"]]
M <- split_dfs[["M"]]



#extra filtering for A
# Regular expression to match columns with "A" as the first capital letter after lowercase letters and numbers
pattern <- "^[a-z0-9]*A"

# Identify columns that do not match the pattern
columns_to_keep <- !grepl(pattern, names(A))

# Subset the dataframe to keep only the desired columns
A <- A[, !columns_to_keep]
A$time <- compartments$time

```

Output processing for multiple solutions.

```{r Output Processing, eval = FALSE }


NamesK <- SBNames
Solutions_all <- list()


# Define the function to split dataframes based on patterns
split_df <- function(df, compartments) {
  # Extract the capital letters (A, R, C, T, M) in column names
  patterns <- unique(gsub("[^ARCTM]", "", colnames(df)))
  
  # Function to adjust the dataframe
  adjust_df <- function(df) {
    df$time <- compartments$time
    return(df)
  }
  
  # Split the dataframe based on the presence of specified capital letters using lapply
  split_dfs <- lapply(patterns, function(pattern) {
    cols <- grepl(pattern, colnames(df))
    adjusted_df <- df[, cols, drop = FALSE]
    adjust_df(adjusted_df)
  })
  
  # Assign names to the split dataframes
  names(split_dfs) <- patterns
  
  return(split_dfs)
}

# Initialize a list to store the results)

# Loop through all solutions in the list
for (i in seq_along(Solutions)) {
  # Convert the current solution to a dataframe
  Solution <- as.data.frame(Solutions[[i]])
  
  # Set column names for the compartments and signals
  colnames(Solution)[2:156] <- NamesK
  
  # Separate signals and matrix compartments
  compartments <- Solution[, 1:156]
  compartments <- compartments %>% select(-matches("U$"))
  
  signals <- Solution[, 157:311]
  colnames(signals) <- NamesK
  signals_total <- data.frame(rowSums(signals))
  signals_total$time <- Solution[, 1]
  
  # Plot emissions over time
  plot1 <- ggplot(signals_total, aes(x = time, y = rowSums.signals.)) + 
    geom_line() +
    labs(title = "Emissions Over Time", x = "Time", y = "Emissions")
  
  print(plot1)  # Use print instead of show in scripts
  
  # Split the compartments dataframe
  split_dfs <- split_df(compartments, compartments)
  
  # Extract the individual split dataframes
  A <- split_dfs[["A"]]
  C <- split_dfs[["C"]]
  T <- split_dfs[["T"]]
  R <- split_dfs[["R"]]
  M <- split_dfs[["M"]]
  
  # Further filtering for A
  pattern <- "^[a-z0-9]*A"
  columns_to_keep <- !grepl(pattern, names(A))
  A <- A[, !columns_to_keep]
  A$time <- compartments$time
  
  
  # Save the results in the list with dynamic names
  Solutions_all[[paste0("Solution_", i)]] <- list(
    A = A,
    C = C,
    T = T,
    R = R,
    M = M
  )
}




```

## Plotting of Output

Below you can create a plot for one output, the chunk after that represents uncertainty based on multiple outputs.

```{r plotting}
# Define the plot_dataframe function
plot_dataframe <- function(df, title) {
  data_to_plot <- tidyr::gather(df, key = "variable", value = "value", -time)
  
  ggplot(data_to_plot, aes(x = time, y = value, color = variable)) +
    geom_line() +
    labs(title = title,
         x = "Time",
         y = "Value")
}

# Named list of dataframes
dataframes <- list(
  A = A,
  C = C,
  T = T,
  R = R,
  M = M
)

# Plot and print each dataframe
plots <- lapply(names(dataframes), function(name) {
  plot_dataframe(dataframes[[name]], name)
})

# Show the plots
for (plot in plots) {
  print(plot)
}
w2C <- C[c("w2CA", "w2CS", "w2CP", "time")]

ggplot(w2C, aes(x = time)) +
  geom_line(aes(y = w2CA, color = "w2CA")) +
  geom_line(aes(y = w2CS, color = "w2CS")) +
  geom_line(aes(y = w2CP, color = "w2CP")) +
  labs(title = "Plot of w2CA, w2CS, and w2CP over time",
       x = "Time",
       y = "Values",
       color = "Legend") +
  theme_minimal()
```

```{r output from all solutions, eval = FALSE}

## seperate dataframes 
# Loop through the Solutions_all list and plot each dataframe
for (i in seq_along(Solutions_all)) {
  solution <- Solutions_all[[i]]
  for (j in names(solution)) {
    plot <- plot_dataframe(solution[[j]], title = j)
    print(plot)
  }
}


# Initialize an empty list to store summary statistics per compartment
summary_stats_per_compartment <- list()

# Loop through each compartment (A, C, T, R, M)
for (compartment in c("A", "C", "T", "R", "M")) {
  compartment_data <- lapply(Solutions_all, function(solution) {
    # Extract the dataframe for the current compartment
    df <- solution[[compartment]]
    df$time <- as.character(df$time) # Ensure time column is character for consistency
    
    # Gather the data for easier processing
    df_gathered <- tidyr::gather(df, key = "variable", value = "value", -time)
    
    return(df_gathered)
  })
  
  # Combine data for all solutions and calculate mean and standard deviation
  combined_data <- bind_rows(compartment_data)
  summary_stats <- combined_data %>%
    group_by(time, variable) %>%
    summarize(mean_value = mean(value),
              sd_value = sd(value))
  
  # Add compartment information
  summary_stats$compartment <- compartment
  
  # Append to the summary_stats_per_compartment list
  summary_stats_per_compartment[[compartment]] <- summary_stats
}

# Combine all compartment summary statistics into one dataframe
summary_stats_final <- bind_rows(summary_stats_per_compartment)

summary_stats_C <- summary_stats_final |>
  filter(compartment == "C")

# Plot for compartment A
ggplot(summary_stats_C, aes(x = as.numeric(time), y = mean_value, group = variable, color = variable)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value, fill = variable), alpha = 0.3) +
  labs(title = "Mass in compartment with Standard Deviation for Compartment C",
       x = "Time",
       y = "Mean Value")
#color = "Variable").


# Create a list to store the plots
plots_list <- list()

# Iterate over compartments
for(compartment in unique(summary_stats_final$compartment)) {
  
  # Subset data for the current compartment
  summary_stats_compartment <- summary_stats_final %>%
    filter(compartment == compartment)
  
  # Get unique variables for the current compartment
  unique_variables <- unique(summary_stats_compartment$variable)
  
  # Create plot for the current compartment
  plot <- ggplot(summary_stats_compartment, aes(x = as.numeric(time), y = mean_value, group = variable, color = variable)) +
    geom_line(data = subset(summary_stats_compartment, variable %in% unique_variables)) +
    geom_ribbon(data = subset(summary_stats_compartment, variable %in% unique_variables), aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value, fill = variable), alpha = 0.3) +
    labs(title = paste("Mass in compartment with Standard Deviation for", compartment),
         x = "Time",
         y = "Mean Value",
         color = "Variable",
         fill = "Variable")
  
  # Store the plot in the list
  plots_list[[compartment]] <- plot
}

# Print each plot
for (i in 1:length(plots_list)) {
  print(plots_list[[i]])
}



```
##From mass to concentration 
In this chunk we will convert the mass output into concentration output by calculating the respective volumes and calculating by this value. 
```{r mass to concentration, eval = FALSE}
Volume <-World$fetchData("Volume")
FRACw <- World$fetchData("FRACw")
FRACa <- World$fetchData("FRACa")
Rho <-World$fetchData("rhoMatrix")


accronym_map <- c("marinesediment" = "sd2",
                  "freshwatersediment" = "sd1",
                  "lakesediment" = "sd0", #SB Excel does not have this compartment. To do: can we turn this off (exclude this compartment) for testing?
                  "agriculturalsoil" = "s2",
                  "naturalsoil" = "s1",
                  "othersoil" = "s3",
                  "air" = "a",
                  "deepocean" = "w3",
                  "sea" = "w2",
                  "river" = "w1",
                  "lake" = "w0", 
                  "cloudwater" = "cw")

accronym_map2 <- c("Arctic" = "A",
                   "Moderate" = "M",
                   "Tropic" = "T",
                   "Continental" = "C",
                   "Regional" = "R")

Volume <- Volume |> mutate(compartment =  paste0(accronym_map[SubCompart], 
                                      accronym_map2[Scale]))
FRACw <- FRACw |> mutate(compartment =  paste0(accronym_map[SubCompart], 
                                               accronym_map2[Scale]))
FRACa <- FRACa |> mutate(compartment =  paste0(accronym_map[SubCompart], 
                                               accronym_map2[Scale]))
Rho <-  Rho |> mutate(compartment =  paste0(accronym_map[SubCompart]))

# List of columns in A that need transformation
CompartmentsConc <-compartments
columns_to_transform <- setdiff(names(CompartmentsConc), "time")
print(columns_to_transform)
# Loop through each column in A and calculate concentrations
for (col in columns_to_transform) {
  # Extract the compartment prefix (e.g., 'aA' from 'aAS')
  compartment <- substr(col, 1, nchar(col) - 1)
  
  # Find the corresponding volume from Volume dataframe
  volume <- Volume$Volume[Volume$compartment == compartment]
  
  # Check if volume was found
  if (length(volume) == 1) {
    # Divide the column values by the volume
    CompartmentsConc[[col]] <- CompartmentsConc[[col]] / volume
  } else {
    warning(paste("Volume not found for compartment", compartment))
  }
}

CompartmentsConc_soil_names <- grep("^s[123]", names(CompartmentsConc), value = TRUE)
CompartmentsConc_soil <- CompartmentsConc[, CompartmentsConc_soil_names]
RhoWater <- Rho |> filter(SubCompart == "river")

 #need both water and soil/sediment
f_Soil.wetweight <- function(CompartmentsConc.soil, # in kg/m3 soil or sediment
                             Fracw,
                             Fraca,
                             RHOsolid){
  CompartmentsConc.soil*1000/(FRACw*RhoWater+(1-FRACw-FRACa)*Rho) # in g/kg (wet) soil
}

#Extract the parameters

compartment_prefixes_scale <- substr(names(CompartmentsConc_soil), 1, 3)
compartment_prefixes <- substr(names(CompartmentsConc_soil), 1, 2)
print(compartment_prefixes)

# Find corresponding FRACw, FRACa, and Rho values
FRACw_values <- FRACw$FRACw[match(compartment_prefixes_scale, FRACw$compartment)]
FRACa_values <- FRACa$FRACa[match(compartment_prefixes_scale, FRACa$compartment)]
Rho_values <- Rho$rhoMatrix[match(compartment_prefixes, Rho$compartment)]
# Retrieve RhoWater value

#Function f_Soil.wetweight
f_Soil.wetweight <- function(CompartmentsConc.soil, Fracw, Fraca, RHOsolid) {
  CompartmentsConc.soil * 1000 / (Fracw * RhoWater + (1 - Fracw - Fraca) * RHOsolid)
}

# Step 3: Apply the function to CompartmentsConc_soil
CompartmentsConc_soil_adjusted <- CompartmentsConc_soil

for (col in 1:ncol(CompartmentsConc_soil)) {
  current_col <- CompartmentsConc_soil[, col]
  current_Fracw <- FRACw_values[col]
  current_Fraca <- FRACa_values[col]
  current_Rho <- Rho_values[col]
  
  CompartmentsConc_soil_adjusted[, col] <- f_Soil.wetweight(current_col, current_Fracw, current_Fraca, current_Rho)
}

# Assigning column names to CompartmentsConc_soil_adjusted
colnames(CompartmentsConc_soil_adjusted) <- colnames(CompartmentsConc_soil)
print(CompartmentsConc_soil_adjusted)


```